{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f2484d1-dd6f-456a-8a70-56b2a5844b48",
   "metadata": {},
   "source": [
    "# 1. Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76b86d3e-3335-4dd3-814a-eb146d10c8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import keras_tuner as kt\n",
    "import json\n",
    "from tensorflow.keras import Input, Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Concatenate, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Nadam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from utils import get_data_path, get_save_path, save_json\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "179d07e8-c818-4775-b0a5-2d0168aac98e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Custom Focal Loss\n",
    "class FocalLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, gamma=2., alpha=0.75, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1. - K.epsilon())\n",
    "        pt = tf.where(tf.equal(y_true, 1), y_pred, 1 - y_pred)\n",
    "        return -K.mean(self.alpha * K.pow(1. - pt, self.gamma) * K.log(pt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38ab807-cc5c-442c-8b94-b00c4dbdc139",
   "metadata": {},
   "source": [
    "# 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ace15879-0088-4ae6-bcf9-2fb7769375f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(get_data_path(\"X_train_deep.csv\"))\n",
    "y_train = pd.read_csv(get_data_path(\"y_train.csv\")).squeeze()\n",
    "X_val = pd.read_csv(get_data_path(\"X_val_deep.csv\"))\n",
    "y_val = pd.read_csv(get_data_path(\"y_val.csv\")).squeeze()\n",
    "\n",
    "input_dim = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4d6ce0-c0f2-4e8e-ab73-e5dceeb378ee",
   "metadata": {},
   "source": [
    "# 3. Dense Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26bfbd26-4525-4a5c-935d-a98d79433d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tunable Feedforward Model\n",
    "def build_dense_model_hp(hp):\n",
    "    batch_size = hp.Choice(\"batch_size\", [32, 64, 128])\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(input_dim,)))\n",
    "\n",
    "    for i in range(hp.Int(\"num_layers\", 2, 6)):\n",
    "        units = hp.Choice(f\"units_{i}\", [32, 64, 128, 256, 512])\n",
    "        activation = hp.Choice(\"activation\", [\"relu\", \"tanh\", \"swish\"])\n",
    "        model.add(Dense(units, activation=activation))\n",
    "        \n",
    "        # L2 regularization hyperparameter\n",
    "        l2_reg = hp.Choice(f\"l2_{i}\", [0.0, 1e-5, 1e-4, 1e-3])\n",
    "\n",
    "        model.add(Dense(\n",
    "            units,\n",
    "            activation=activation,\n",
    "            kernel_regularizer=l2(l2_reg)\n",
    "        ))\n",
    "        \n",
    "        if hp.Boolean(f\"use_bn_{i}\"):\n",
    "            model.add(BatchNormalization())\n",
    "\n",
    "        dropout_rate = hp.Choice(f\"dropout_{i}\", [0.3, 0.4, 0.5])\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    optimizer_choice = hp.Choice(\"optimizer\", [\"adam\", \"rmsprop\", \"sgd\", \"nadam\"])\n",
    "    lr = hp.Float(\"learning_rate\", 1e-4, 1e-2, sampling=\"log\")\n",
    "\n",
    "    if optimizer_choice == \"adam\":\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "    elif optimizer_choice == \"sgd\":\n",
    "        optimizer = SGD(learning_rate=lr)\n",
    "    elif optimizer_choice == \"nadam\":\n",
    "        optimizer = Nadam(learning_rate=lr)\n",
    "    else:\n",
    "        optimizer = RMSprop(learning_rate=lr)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=FocalLoss(alpha=0.75),\n",
    "        metrics=[\n",
    "            \"accuracy\",\n",
    "            tf.keras.metrics.Recall(name=\"recall\"),\n",
    "            tf.keras.metrics.AUC(name=\"AUC\", curve=\"ROC\"),\n",
    "            tf.keras.metrics.AUC(name=\"PR_AUC\", curve=\"PR\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3994d66b-00e2-45ad-a6f8-1b649e066626",
   "metadata": {},
   "source": [
    "# 4. Create Bayesian Optimization Tuner "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c54044ae-cc67-46ac-b13c-3f53c0e84e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from tuner_dir/dense_model_focal_tuned/tuner0.json\n"
     ]
    }
   ],
   "source": [
    "bayes_tuner = kt.BayesianOptimization(\n",
    "    build_dense_model_hp,\n",
    "    objective=\"val_AUC\",\n",
    "    max_trials=30,\n",
    "    directory=\"tuner_dir\",\n",
    "    project_name=\"dense_model_focal_tuned\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237786a5-a27c-4929-9ace-239075c5f55b",
   "metadata": {},
   "source": [
    "# 5. Start Automated Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04d8c4c1-cc99-4cd5-bf0a-38c1144b2746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EarlyStopping\n",
    "early_stop = EarlyStopping(\n",
    "    monitor=\"val_AUC\", \n",
    "    mode=\"max\",\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "# Start tuning\n",
    "# Here, EarlyStopping is used to find the best epoch\n",
    "# After saving the model and parameters, we will use the same early stopping or retrain to that epoch in notebook 04\n",
    "\n",
    "bayes_tuner.search(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    callbacks=[early_stop]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cc3ea8-4930-45ae-9076-db997d05a883",
   "metadata": {},
   "source": [
    "# 6. Save Best Model and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fea59583-84d8-463e-b5a4-5c085a734adf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in tuner_dir/dense_model_focal_tuned\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_AUC\", direction=\"max\")\n",
      "\n",
      "Trial 09 summary\n",
      "Hyperparameters:\n",
      "batch_size: 32\n",
      "num_layers: 3\n",
      "units_0: 128\n",
      "activation: swish\n",
      "l2_0: 1e-05\n",
      "use_bn_0: True\n",
      "dropout_0: 0.3\n",
      "units_1: 128\n",
      "l2_1: 0.0001\n",
      "use_bn_1: True\n",
      "dropout_1: 0.3\n",
      "optimizer: adam\n",
      "learning_rate: 0.0012422772048365925\n",
      "units_2: 128\n",
      "l2_2: 0.0\n",
      "use_bn_2: False\n",
      "dropout_2: 0.5\n",
      "units_3: 64\n",
      "l2_3: 0.001\n",
      "use_bn_3: False\n",
      "dropout_3: 0.3\n",
      "units_4: 32\n",
      "l2_4: 1e-05\n",
      "use_bn_4: True\n",
      "dropout_4: 0.4\n",
      "Score: 0.6405916810035706\n",
      "\n",
      "Trial 07 summary\n",
      "Hyperparameters:\n",
      "batch_size: 32\n",
      "num_layers: 3\n",
      "units_0: 256\n",
      "activation: relu\n",
      "l2_0: 1e-05\n",
      "use_bn_0: True\n",
      "dropout_0: 0.3\n",
      "units_1: 128\n",
      "l2_1: 0.0001\n",
      "use_bn_1: True\n",
      "dropout_1: 0.3\n",
      "optimizer: rmsprop\n",
      "learning_rate: 0.005808921978016473\n",
      "units_2: 256\n",
      "l2_2: 0.001\n",
      "use_bn_2: False\n",
      "dropout_2: 0.4\n",
      "units_3: 128\n",
      "l2_3: 1e-05\n",
      "use_bn_3: False\n",
      "dropout_3: 0.5\n",
      "units_4: 256\n",
      "l2_4: 1e-05\n",
      "use_bn_4: True\n",
      "dropout_4: 0.5\n",
      "Score: 0.639795184135437\n",
      "\n",
      "Trial 28 summary\n",
      "Hyperparameters:\n",
      "batch_size: 64\n",
      "num_layers: 3\n",
      "units_0: 128\n",
      "activation: relu\n",
      "l2_0: 1e-05\n",
      "use_bn_0: False\n",
      "dropout_0: 0.5\n",
      "units_1: 64\n",
      "l2_1: 1e-05\n",
      "use_bn_1: False\n",
      "dropout_1: 0.4\n",
      "optimizer: nadam\n",
      "learning_rate: 0.0010760280342821636\n",
      "units_2: 256\n",
      "l2_2: 0.001\n",
      "use_bn_2: False\n",
      "dropout_2: 0.4\n",
      "units_3: 256\n",
      "l2_3: 0.001\n",
      "use_bn_3: True\n",
      "dropout_3: 0.3\n",
      "units_4: 512\n",
      "l2_4: 0.001\n",
      "use_bn_4: False\n",
      "dropout_4: 0.3\n",
      "units_5: 256\n",
      "l2_5: 0.0\n",
      "use_bn_5: False\n",
      "dropout_5: 0.3\n",
      "Score: 0.6389044523239136\n",
      "\n",
      "Trial 10 summary\n",
      "Hyperparameters:\n",
      "batch_size: 32\n",
      "num_layers: 2\n",
      "units_0: 256\n",
      "activation: swish\n",
      "l2_0: 0.0001\n",
      "use_bn_0: True\n",
      "dropout_0: 0.3\n",
      "units_1: 64\n",
      "l2_1: 0.0\n",
      "use_bn_1: True\n",
      "dropout_1: 0.5\n",
      "optimizer: nadam\n",
      "learning_rate: 0.005719302395363331\n",
      "units_2: 512\n",
      "l2_2: 0.0\n",
      "use_bn_2: True\n",
      "dropout_2: 0.3\n",
      "units_3: 256\n",
      "l2_3: 0.001\n",
      "use_bn_3: False\n",
      "dropout_3: 0.5\n",
      "units_4: 128\n",
      "l2_4: 1e-05\n",
      "use_bn_4: False\n",
      "dropout_4: 0.3\n",
      "Score: 0.6387714147567749\n",
      "\n",
      "Trial 15 summary\n",
      "Hyperparameters:\n",
      "batch_size: 64\n",
      "num_layers: 2\n",
      "units_0: 128\n",
      "activation: swish\n",
      "l2_0: 0.0\n",
      "use_bn_0: True\n",
      "dropout_0: 0.3\n",
      "units_1: 32\n",
      "l2_1: 0.0\n",
      "use_bn_1: True\n",
      "dropout_1: 0.3\n",
      "optimizer: adam\n",
      "learning_rate: 0.00014436486952248186\n",
      "units_2: 128\n",
      "l2_2: 1e-05\n",
      "use_bn_2: False\n",
      "dropout_2: 0.5\n",
      "units_3: 32\n",
      "l2_3: 0.0001\n",
      "use_bn_3: True\n",
      "dropout_3: 0.4\n",
      "units_4: 128\n",
      "l2_4: 0.0001\n",
      "use_bn_4: False\n",
      "dropout_4: 0.3\n",
      "units_5: 128\n",
      "l2_5: 0.0001\n",
      "use_bn_5: True\n",
      "dropout_5: 0.5\n",
      "Score: 0.6375243067741394\n",
      "\n",
      "Trial 25 summary\n",
      "Hyperparameters:\n",
      "batch_size: 32\n",
      "num_layers: 6\n",
      "units_0: 256\n",
      "activation: relu\n",
      "l2_0: 0.0\n",
      "use_bn_0: True\n",
      "dropout_0: 0.3\n",
      "units_1: 256\n",
      "l2_1: 1e-05\n",
      "use_bn_1: True\n",
      "dropout_1: 0.4\n",
      "optimizer: rmsprop\n",
      "learning_rate: 0.003060764805239088\n",
      "units_2: 128\n",
      "l2_2: 1e-05\n",
      "use_bn_2: False\n",
      "dropout_2: 0.4\n",
      "units_3: 128\n",
      "l2_3: 0.0\n",
      "use_bn_3: False\n",
      "dropout_3: 0.4\n",
      "units_4: 64\n",
      "l2_4: 1e-05\n",
      "use_bn_4: False\n",
      "dropout_4: 0.4\n",
      "units_5: 256\n",
      "l2_5: 0.001\n",
      "use_bn_5: True\n",
      "dropout_5: 0.3\n",
      "Score: 0.6368758082389832\n",
      "\n",
      "Trial 14 summary\n",
      "Hyperparameters:\n",
      "batch_size: 128\n",
      "num_layers: 6\n",
      "units_0: 256\n",
      "activation: swish\n",
      "l2_0: 0.0\n",
      "use_bn_0: True\n",
      "dropout_0: 0.4\n",
      "units_1: 256\n",
      "l2_1: 0.0\n",
      "use_bn_1: True\n",
      "dropout_1: 0.5\n",
      "optimizer: nadam\n",
      "learning_rate: 0.0010475393858135431\n",
      "units_2: 512\n",
      "l2_2: 0.0\n",
      "use_bn_2: False\n",
      "dropout_2: 0.5\n",
      "units_3: 512\n",
      "l2_3: 0.0001\n",
      "use_bn_3: False\n",
      "dropout_3: 0.3\n",
      "units_4: 128\n",
      "l2_4: 0.0001\n",
      "use_bn_4: False\n",
      "dropout_4: 0.3\n",
      "units_5: 32\n",
      "l2_5: 0.0\n",
      "use_bn_5: False\n",
      "dropout_5: 0.3\n",
      "Score: 0.6363657712936401\n",
      "\n",
      "Trial 06 summary\n",
      "Hyperparameters:\n",
      "batch_size: 64\n",
      "num_layers: 5\n",
      "units_0: 128\n",
      "activation: swish\n",
      "l2_0: 0.001\n",
      "use_bn_0: True\n",
      "dropout_0: 0.4\n",
      "units_1: 256\n",
      "l2_1: 0.001\n",
      "use_bn_1: False\n",
      "dropout_1: 0.4\n",
      "optimizer: rmsprop\n",
      "learning_rate: 0.00019937773888529741\n",
      "units_2: 512\n",
      "l2_2: 0.0001\n",
      "use_bn_2: False\n",
      "dropout_2: 0.3\n",
      "units_3: 64\n",
      "l2_3: 1e-05\n",
      "use_bn_3: True\n",
      "dropout_3: 0.3\n",
      "units_4: 128\n",
      "l2_4: 0.0\n",
      "use_bn_4: True\n",
      "dropout_4: 0.5\n",
      "Score: 0.6350924372673035\n",
      "\n",
      "Trial 19 summary\n",
      "Hyperparameters:\n",
      "batch_size: 64\n",
      "num_layers: 5\n",
      "units_0: 32\n",
      "activation: relu\n",
      "l2_0: 1e-05\n",
      "use_bn_0: False\n",
      "dropout_0: 0.4\n",
      "units_1: 256\n",
      "l2_1: 0.0001\n",
      "use_bn_1: True\n",
      "dropout_1: 0.3\n",
      "optimizer: adam\n",
      "learning_rate: 0.00040508467510031853\n",
      "units_2: 128\n",
      "l2_2: 0.0001\n",
      "use_bn_2: False\n",
      "dropout_2: 0.3\n",
      "units_3: 512\n",
      "l2_3: 0.001\n",
      "use_bn_3: False\n",
      "dropout_3: 0.4\n",
      "units_4: 32\n",
      "l2_4: 1e-05\n",
      "use_bn_4: True\n",
      "dropout_4: 0.3\n",
      "units_5: 512\n",
      "l2_5: 0.001\n",
      "use_bn_5: False\n",
      "dropout_5: 0.4\n",
      "Score: 0.6348825097084045\n",
      "\n",
      "Trial 04 summary\n",
      "Hyperparameters:\n",
      "batch_size: 64\n",
      "num_layers: 5\n",
      "units_0: 256\n",
      "activation: relu\n",
      "l2_0: 0.0\n",
      "use_bn_0: True\n",
      "dropout_0: 0.4\n",
      "units_1: 256\n",
      "l2_1: 0.0\n",
      "use_bn_1: True\n",
      "dropout_1: 0.4\n",
      "optimizer: nadam\n",
      "learning_rate: 0.00029162547269851575\n",
      "units_2: 512\n",
      "l2_2: 1e-05\n",
      "use_bn_2: True\n",
      "dropout_2: 0.5\n",
      "units_3: 64\n",
      "l2_3: 0.001\n",
      "use_bn_3: True\n",
      "dropout_3: 0.3\n",
      "units_4: 32\n",
      "l2_4: 0.0\n",
      "use_bn_4: False\n",
      "dropout_4: 0.3\n",
      "Score: 0.634675145149231\n",
      "\n",
      "Best Hyperparameter Combination:\n",
      "{'batch_size': 32, 'num_layers': 3, 'units_0': 128, 'activation': 'swish', 'l2_0': 1e-05, 'use_bn_0': True, 'dropout_0': 0.3, 'units_1': 128, 'l2_1': 0.0001, 'use_bn_1': True, 'dropout_1': 0.3, 'optimizer': 'adam', 'learning_rate': 0.0012422772048365925, 'units_2': 128, 'l2_2': 0.0, 'use_bn_2': False, 'dropout_2': 0.5, 'units_3': 64, 'l2_3': 0.001, 'use_bn_3': False, 'dropout_3': 0.3, 'units_4': 32, 'l2_4': 1e-05, 'use_bn_4': True, 'dropout_4': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/actl3143/lib/python3.11/site-packages/keras/src/saving/saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 38 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "# Save the best model to the specified folder\n",
    "best_model = bayes_tuner.get_best_models(1)[0]\n",
    "best_model.save(get_save_path(\"best_model_dense.keras\"))\n",
    "\n",
    "# Save the best hyperparameters as JSON\n",
    "best_hp = bayes_tuner.get_best_hyperparameters(1)[0]\n",
    "save_json(best_hp.values, \"best_hyperparameters_dense.json\")\n",
    "\n",
    "bayes_tuner.results_summary()\n",
    "print(\"\\nBest Hyperparameter Combination:\")\n",
    "print(best_hp.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346ccf4d-44cc-4725-9cbf-96ecf12d317a",
   "metadata": {},
   "source": [
    "# 7.Wide & Deep tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cbffa12-c9af-4709-af2a-d022acf55a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_wide_and_deep_model_hp(hp):\n",
    "    batch_size = hp.Choice(\"batch_size\", [32, 64, 128]) \n",
    "    \n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    \n",
    "    # Deep Branch\n",
    "    deep = input_layer\n",
    "    for i in range(hp.Int(\"num_layers\", 2, 6)):\n",
    "        units = hp.Choice(f\"units_{i}\", [64, 128, 256, 512])\n",
    "        activation = hp.Choice(\"activation\", [\"relu\", \"tanh\", \"swish\"])\n",
    "        deep = Dense(units, activation=activation)(deep)\n",
    "        l2_reg = hp.Choice(f\"l2_{i}\", [0.0, 1e-5, 1e-4, 1e-3])  # L2\n",
    "        \n",
    "        deep = Dense(units, activation=activation, kernel_regularizer=l2(l2_reg))(deep)\n",
    "\n",
    "        if hp.Boolean(f\"use_bn_{i}\"):\n",
    "            deep = BatchNormalization()(deep)\n",
    "\n",
    "        dropout_rate = hp.Choice(f\"dropout_{i}\", [0.3, 0.4, 0.5])\n",
    "        deep = Dropout(dropout_rate)(deep)\n",
    "\n",
    "    # Wide Branch + Concatenation\n",
    "    combined = Concatenate()([input_layer, deep])\n",
    "    output = Dense(1, activation=\"sigmoid\")(combined)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "    # Optimizer Settings\n",
    "    optimizer_name = hp.Choice(\"optimizer\", [\"adam\", \"rmsprop\", \"sgd\", \"nadam\"])\n",
    "    lr = hp.Float(\"learning_rate\", min_value=1e-5, max_value=1e-2, sampling=\"log\")\n",
    "\n",
    "    if optimizer_name == \"adam\":\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "    elif optimizer_name == \"rmsprop\":\n",
    "        optimizer = RMSprop(learning_rate=lr)\n",
    "    elif optimizer_name == \"sgd\":\n",
    "        optimizer = SGD(learning_rate=lr)\n",
    "    else:\n",
    "        optimizer = Nadam(learning_rate=lr)\n",
    "\n",
    "    # Loss Function (can switch focal loss) \n",
    "    loss_type = hp.Choice(\"loss_fn\", [\"binary_crossentropy\", \"focal\"])\n",
    "    loss_fn = \"binary_crossentropy\" if loss_type == \"binary_crossentropy\" else FocalLoss()\n",
    "\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=FocalLoss(alpha=0.75),\n",
    "        metrics=[\n",
    "            \"accuracy\",\n",
    "            tf.keras.metrics.Recall(name=\"recall\"),\n",
    "            tf.keras.metrics.AUC(name=\"AUC\", curve=\"ROC\"),\n",
    "            tf.keras.metrics.AUC(name=\"PR_AUC\", curve=\"PR\")\n",
    "        ]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8a10d8-93c0-4436-ac2b-fa0f7beb9e0e",
   "metadata": {},
   "source": [
    "# 8. Bayesian Tuner for Wide & Deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e99649d-db26-4271-b6ba-d21f772cd2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from tuner_dir/wide_deep_model_tuning/tuner0.json\n"
     ]
    }
   ],
   "source": [
    "tuner_wd = kt.BayesianOptimization(\n",
    "    build_wide_and_deep_model_hp,\n",
    "    objective=\"val_AUC\",\n",
    "    max_trials=30,\n",
    "    directory=\"tuner_dir\",\n",
    "    project_name=\"wide_deep_model_tuning\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c843ef0-0dde-4dbe-a5b4-a5007e920697",
   "metadata": {},
   "source": [
    "# 9. Start Search (Wide & Deep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "274daba9-8f14-46f9-abba-a8975abc68f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_wd.search(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bd23a4-fd83-435b-a4c1-3daecd7bf556",
   "metadata": {},
   "source": [
    "# 10. Save Best Wide & Deep Model and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94c23a6c-86cf-4432-a9fa-7b6ac1dc03df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in tuner_dir/wide_deep_model_tuning\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_AUC\", direction=\"max\")\n",
      "\n",
      "Trial 21 summary\n",
      "Hyperparameters:\n",
      "batch_size: 128\n",
      "num_layers: 6\n",
      "units_0: 128\n",
      "activation: swish\n",
      "l2_0: 1e-05\n",
      "use_bn_0: True\n",
      "dropout_0: 0.3\n",
      "units_1: 512\n",
      "l2_1: 1e-05\n",
      "use_bn_1: True\n",
      "dropout_1: 0.4\n",
      "optimizer: nadam\n",
      "learning_rate: 0.0002537689381953388\n",
      "loss_fn: binary_crossentropy\n",
      "units_2: 512\n",
      "l2_2: 0.0001\n",
      "use_bn_2: True\n",
      "dropout_2: 0.5\n",
      "units_3: 128\n",
      "l2_3: 0.0\n",
      "use_bn_3: True\n",
      "dropout_3: 0.5\n",
      "units_4: 512\n",
      "l2_4: 0.001\n",
      "use_bn_4: False\n",
      "dropout_4: 0.5\n",
      "units_5: 64\n",
      "l2_5: 0.001\n",
      "use_bn_5: False\n",
      "dropout_5: 0.5\n",
      "Score: 0.6407451629638672\n",
      "\n",
      "Trial 12 summary\n",
      "Hyperparameters:\n",
      "batch_size: 32\n",
      "num_layers: 6\n",
      "units_0: 128\n",
      "activation: relu\n",
      "l2_0: 0.0001\n",
      "use_bn_0: True\n",
      "dropout_0: 0.3\n",
      "units_1: 512\n",
      "l2_1: 0.001\n",
      "use_bn_1: True\n",
      "dropout_1: 0.3\n",
      "optimizer: adam\n",
      "learning_rate: 0.00040038051991221\n",
      "loss_fn: binary_crossentropy\n",
      "units_2: 256\n",
      "l2_2: 1e-05\n",
      "use_bn_2: False\n",
      "dropout_2: 0.5\n",
      "units_3: 256\n",
      "l2_3: 0.0\n",
      "use_bn_3: False\n",
      "dropout_3: 0.3\n",
      "units_4: 128\n",
      "l2_4: 0.0001\n",
      "use_bn_4: False\n",
      "dropout_4: 0.4\n",
      "units_5: 256\n",
      "l2_5: 0.001\n",
      "use_bn_5: False\n",
      "dropout_5: 0.3\n",
      "Score: 0.6347813606262207\n",
      "\n",
      "Trial 27 summary\n",
      "Hyperparameters:\n",
      "batch_size: 128\n",
      "num_layers: 4\n",
      "units_0: 256\n",
      "activation: relu\n",
      "l2_0: 0.0\n",
      "use_bn_0: True\n",
      "dropout_0: 0.3\n",
      "units_1: 512\n",
      "l2_1: 0.0\n",
      "use_bn_1: True\n",
      "dropout_1: 0.4\n",
      "optimizer: nadam\n",
      "learning_rate: 3.675382829883336e-05\n",
      "loss_fn: binary_crossentropy\n",
      "units_2: 512\n",
      "l2_2: 1e-05\n",
      "use_bn_2: False\n",
      "dropout_2: 0.3\n",
      "units_3: 512\n",
      "l2_3: 0.0001\n",
      "use_bn_3: True\n",
      "dropout_3: 0.4\n",
      "units_4: 512\n",
      "l2_4: 0.0\n",
      "use_bn_4: True\n",
      "dropout_4: 0.4\n",
      "units_5: 128\n",
      "l2_5: 0.0\n",
      "use_bn_5: False\n",
      "dropout_5: 0.5\n",
      "Score: 0.6314889192581177\n",
      "\n",
      "Trial 07 summary\n",
      "Hyperparameters:\n",
      "batch_size: 32\n",
      "num_layers: 6\n",
      "units_0: 512\n",
      "activation: swish\n",
      "l2_0: 0.0001\n",
      "use_bn_0: False\n",
      "dropout_0: 0.4\n",
      "units_1: 256\n",
      "l2_1: 0.0\n",
      "use_bn_1: True\n",
      "dropout_1: 0.3\n",
      "optimizer: nadam\n",
      "learning_rate: 1.1132029799919481e-05\n",
      "loss_fn: focal\n",
      "units_2: 256\n",
      "l2_2: 0.0001\n",
      "use_bn_2: True\n",
      "dropout_2: 0.5\n",
      "units_3: 512\n",
      "l2_3: 0.001\n",
      "use_bn_3: False\n",
      "dropout_3: 0.3\n",
      "units_4: 256\n",
      "l2_4: 0.0\n",
      "use_bn_4: False\n",
      "dropout_4: 0.5\n",
      "units_5: 256\n",
      "l2_5: 1e-05\n",
      "use_bn_5: False\n",
      "dropout_5: 0.5\n",
      "Score: 0.6309505105018616\n",
      "\n",
      "Trial 09 summary\n",
      "Hyperparameters:\n",
      "batch_size: 128\n",
      "num_layers: 2\n",
      "units_0: 256\n",
      "activation: tanh\n",
      "l2_0: 0.0001\n",
      "use_bn_0: True\n",
      "dropout_0: 0.3\n",
      "units_1: 128\n",
      "l2_1: 0.0\n",
      "use_bn_1: False\n",
      "dropout_1: 0.5\n",
      "optimizer: rmsprop\n",
      "learning_rate: 0.0003360571506274256\n",
      "loss_fn: binary_crossentropy\n",
      "units_2: 128\n",
      "l2_2: 1e-05\n",
      "use_bn_2: False\n",
      "dropout_2: 0.3\n",
      "units_3: 512\n",
      "l2_3: 1e-05\n",
      "use_bn_3: True\n",
      "dropout_3: 0.5\n",
      "units_4: 64\n",
      "l2_4: 0.001\n",
      "use_bn_4: True\n",
      "dropout_4: 0.4\n",
      "units_5: 64\n",
      "l2_5: 0.0\n",
      "use_bn_5: False\n",
      "dropout_5: 0.3\n",
      "Score: 0.6299980282783508\n",
      "\n",
      "Trial 29 summary\n",
      "Hyperparameters:\n",
      "batch_size: 128\n",
      "num_layers: 2\n",
      "units_0: 512\n",
      "activation: relu\n",
      "l2_0: 1e-05\n",
      "use_bn_0: True\n",
      "dropout_0: 0.4\n",
      "units_1: 512\n",
      "l2_1: 1e-05\n",
      "use_bn_1: True\n",
      "dropout_1: 0.5\n",
      "optimizer: sgd\n",
      "learning_rate: 0.003779359776404597\n",
      "loss_fn: binary_crossentropy\n",
      "units_2: 64\n",
      "l2_2: 0.0001\n",
      "use_bn_2: True\n",
      "dropout_2: 0.3\n",
      "units_3: 128\n",
      "l2_3: 1e-05\n",
      "use_bn_3: True\n",
      "dropout_3: 0.3\n",
      "units_4: 64\n",
      "l2_4: 0.0001\n",
      "use_bn_4: False\n",
      "dropout_4: 0.4\n",
      "units_5: 128\n",
      "l2_5: 0.001\n",
      "use_bn_5: True\n",
      "dropout_5: 0.4\n",
      "Score: 0.6279154419898987\n",
      "\n",
      "Trial 25 summary\n",
      "Hyperparameters:\n",
      "batch_size: 32\n",
      "num_layers: 4\n",
      "units_0: 128\n",
      "activation: tanh\n",
      "l2_0: 1e-05\n",
      "use_bn_0: True\n",
      "dropout_0: 0.4\n",
      "units_1: 256\n",
      "l2_1: 0.0\n",
      "use_bn_1: True\n",
      "dropout_1: 0.4\n",
      "optimizer: adam\n",
      "learning_rate: 0.00016335340250435296\n",
      "loss_fn: focal\n",
      "units_2: 128\n",
      "l2_2: 0.0001\n",
      "use_bn_2: False\n",
      "dropout_2: 0.3\n",
      "units_3: 512\n",
      "l2_3: 1e-05\n",
      "use_bn_3: True\n",
      "dropout_3: 0.3\n",
      "units_4: 128\n",
      "l2_4: 1e-05\n",
      "use_bn_4: False\n",
      "dropout_4: 0.3\n",
      "units_5: 64\n",
      "l2_5: 0.0001\n",
      "use_bn_5: False\n",
      "dropout_5: 0.5\n",
      "Score: 0.6278746128082275\n",
      "\n",
      "Trial 13 summary\n",
      "Hyperparameters:\n",
      "batch_size: 128\n",
      "num_layers: 6\n",
      "units_0: 256\n",
      "activation: relu\n",
      "l2_0: 0.0001\n",
      "use_bn_0: True\n",
      "dropout_0: 0.4\n",
      "units_1: 256\n",
      "l2_1: 1e-05\n",
      "use_bn_1: False\n",
      "dropout_1: 0.3\n",
      "optimizer: nadam\n",
      "learning_rate: 3.841017210511828e-05\n",
      "loss_fn: focal\n",
      "units_2: 64\n",
      "l2_2: 1e-05\n",
      "use_bn_2: True\n",
      "dropout_2: 0.3\n",
      "units_3: 64\n",
      "l2_3: 1e-05\n",
      "use_bn_3: False\n",
      "dropout_3: 0.3\n",
      "units_4: 512\n",
      "l2_4: 1e-05\n",
      "use_bn_4: True\n",
      "dropout_4: 0.4\n",
      "units_5: 512\n",
      "l2_5: 0.0\n",
      "use_bn_5: True\n",
      "dropout_5: 0.4\n",
      "Score: 0.6269897222518921\n",
      "\n",
      "Trial 02 summary\n",
      "Hyperparameters:\n",
      "batch_size: 64\n",
      "num_layers: 3\n",
      "units_0: 128\n",
      "activation: relu\n",
      "l2_0: 0.0\n",
      "use_bn_0: False\n",
      "dropout_0: 0.5\n",
      "units_1: 128\n",
      "l2_1: 0.0001\n",
      "use_bn_1: True\n",
      "dropout_1: 0.4\n",
      "optimizer: rmsprop\n",
      "learning_rate: 3.727954158460794e-05\n",
      "loss_fn: binary_crossentropy\n",
      "units_2: 64\n",
      "l2_2: 1e-05\n",
      "use_bn_2: False\n",
      "dropout_2: 0.3\n",
      "units_3: 512\n",
      "l2_3: 0.0001\n",
      "use_bn_3: True\n",
      "dropout_3: 0.3\n",
      "Score: 0.6267645359039307\n",
      "\n",
      "Trial 16 summary\n",
      "Hyperparameters:\n",
      "batch_size: 32\n",
      "num_layers: 4\n",
      "units_0: 128\n",
      "activation: relu\n",
      "l2_0: 0.0001\n",
      "use_bn_0: True\n",
      "dropout_0: 0.4\n",
      "units_1: 256\n",
      "l2_1: 1e-05\n",
      "use_bn_1: True\n",
      "dropout_1: 0.4\n",
      "optimizer: adam\n",
      "learning_rate: 4.156437917987673e-05\n",
      "loss_fn: binary_crossentropy\n",
      "units_2: 64\n",
      "l2_2: 0.0001\n",
      "use_bn_2: True\n",
      "dropout_2: 0.3\n",
      "units_3: 256\n",
      "l2_3: 1e-05\n",
      "use_bn_3: True\n",
      "dropout_3: 0.4\n",
      "units_4: 256\n",
      "l2_4: 0.0\n",
      "use_bn_4: True\n",
      "dropout_4: 0.5\n",
      "units_5: 256\n",
      "l2_5: 0.0001\n",
      "use_bn_5: False\n",
      "dropout_5: 0.3\n",
      "Score: 0.6258639693260193\n",
      "Best Wide & Deep Hyperparameters:\n",
      "{'batch_size': 128, 'num_layers': 6, 'units_0': 128, 'activation': 'swish', 'l2_0': 1e-05, 'use_bn_0': True, 'dropout_0': 0.3, 'units_1': 512, 'l2_1': 1e-05, 'use_bn_1': True, 'dropout_1': 0.4, 'optimizer': 'nadam', 'learning_rate': 0.0002537689381953388, 'loss_fn': 'binary_crossentropy', 'units_2': 512, 'l2_2': 0.0001, 'use_bn_2': True, 'dropout_2': 0.5, 'units_3': 128, 'l2_3': 0.0, 'use_bn_3': True, 'dropout_3': 0.5, 'units_4': 512, 'l2_4': 0.001, 'use_bn_4': False, 'dropout_4': 0.5, 'units_5': 64, 'l2_5': 0.001, 'use_bn_5': False, 'dropout_5': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/actl3143/lib/python3.11/site-packages/keras/src/saving/saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'nadam', because it has 2 variables whereas the saved optimizer has 71 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "best_wd_model = tuner_wd.get_best_models(1)[0]\n",
    "best_wd_model.save(get_save_path(\"best_model_widedeep.keras\"))\n",
    "\n",
    "best_wd_hp = tuner_wd.get_best_hyperparameters(1)[0]\n",
    "save_json(best_wd_hp.values, \"best_hyperparameters_widedeep.json\")\n",
    "\n",
    "tuner_wd.results_summary()\n",
    "print(\"Best Wide & Deep Hyperparameters:\")\n",
    "print(best_wd_hp.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a89e49-34ac-46ab-8ab6-f4137c027289",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ACTL3143)",
   "language": "python",
   "name": "actl3143"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
